# Session 20250112_100000
Project: Zwift Race Finder

## Work Done

### OCR Strategy Research & Documentation
- **Researched AI/LLM tools** for automatic UI region detection
  - Investigated ScreenAI, Groq Vision API, Hugging Face, Together AI
  - Compared free tiers and capabilities
  - Evaluated Ollama + Llama 3.2 Vision for local processing

- **Revised OCR strategy** from complex cloud-based to community-driven approach
  - Key insight: UI regions are stable per Zwift version + resolution
  - Shifted focus from perfect name OCR to rider order tracking
  - Designed community configuration file sharing via GitHub PRs

- **Created comprehensive documentation**:
  - `wip-claude/20250112_100000_ocr_comprehensive_strategy.md` - Complete strategy document
  - `tools/ocr/CALIBRATION_GUIDE.md` - Step-by-step calibration instructions
  - Added `record-monitor2.ps1` acquisition method to OCR README

### Recording Analysis
- Analyzed latest recording: `2025-06-10_16-32-20`
  - 2.2GB video, 5,602 PNG frames (~93 minutes)
  - Successfully tested OCR extraction on sample frames
  - Confirmed telemetry and leaderboard detection working

### Key Design Decisions
1. **Community-driven configs** instead of per-user calibration
2. **Focus on rider ordering** with fuzzy matching vs perfect names
3. **Three calibration methods**: Groq API (free), Ollama local, Manual
4. **Simple runtime**: Load config → Fast OCR → Track positions

## Failed Approaches
- Initially considered complex periodic validation with cloud APIs
- Abandoned local Llama 3.2 Vision due to 12GB memory requirement on 16GB system

## Commits
```
c33f26c chore: update .gitignore for mutation testing and Python cache
6f34116 Merge pull request #2 from jchidley/ocr
972d7a7 test(ocr): improve test coverage based on mutation analysis
fb34690 docs: consolidate testing documentation into comprehensive guide
0e30a0c test(ocr): comprehensive OCR testing implementation
```

## Technical Insights

### UI Stability Insight
**Discovery**: Zwift UI element positions remain constant for a given version and screen resolution combination. This means calibration only needs to be done once per configuration, not per user.
**Impact**: Enables community-driven configuration sharing via GitHub PRs
**Application**: Create `ocr-configs/` directory with JSON files like `1920x1080_v1.67.0.json`

### Rider Order vs Name Accuracy
**Discovery**: For race analysis, knowing rider positions matters more than perfect name OCR. "J.Ch1dley" vs "J.Chidley" represents the same rider.
**Impact**: Can use fuzzy matching and position tracking instead of expensive validation
**Application**: Focus on maintaining consistent rider ordering across frames

### Free Tier Vision APIs
**Discovery**: Groq offers free tier vision API with Llama 3.2 Vision support, suitable for occasional calibration tasks
**Impact**: Contributors can create high-quality configs without cost
**Application**: Recommended calibration method using `calibrate_with_vision.py` script